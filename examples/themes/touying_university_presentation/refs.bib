@book{abiteboul1995,
  title = {Foundations of {{Databases}}},
  author = {Abiteboul, Serge and Hull, Richard and Vianu, Victor},
  year = 1995,
  publisher = {Addison-Wesley},
  abstract = {This work presents comprehensive coverage of the foundations and theory of database systems. It is a reference to both classical material and advanced topics, bringing together many subjects including up-to-date coverage of object-oriented and logic databases. Numerous exercises are provided at three levels of difficulty. The book is intended for use by database professionals at all levels of experience, and graduate and senior level students in Advanced Theory of Databases.},
  googlebooks = {HN9QAAAAMAAJ},
  isbn = {978-0-201-53771-0},
  langid = {english},
  keywords = {Computers / Business & Productivity Software / Databases,Computers / Database Administration & Management,Computers / System Administration / Storage & Retrieval},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Abiteboul et al. - 1995 - Foundations of Databases.pdf}
}

@misc{agrawal2025,
  title = {Query-{{Aware Graph Neural Networks}} for {{Enhanced Retrieval-Augmented Generation}}},
  author = {Agrawal, Vibhor and Wang, Fay and Puri, Rishi},
  year = 2025,
  month = jul,
  number = {arXiv:2508.05647},
  eprint = {2508.05647},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.05647},
  urldate = {2026-01-25},
  abstract = {We present a novel graph neural network (GNN) architecture for retrieval-augmented generation (RAG) that leverages query-aware attention mechanisms and learned scoring heads to improve retrieval accuracy on complex, multi-hop questions. Unlike traditional dense retrieval methods that treat documents as independent entities, our approach constructs per-episode knowledge graphs that capture both sequential and semantic relationships between text chunks. We introduce an Enhanced Graph Attention Network with query-guided pooling that dynamically focuses on relevant parts of the graph based on user queries. Experimental results demonstrate that our approach significantly outperforms standard dense retrievers on complex question answering tasks, particularly for questions requiring multi-document reasoning. Our implementation leverages PyTorch Geometric for efficient processing of graph-structured data, enabling scalable deployment in production retrieval systems},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Agrawal et al. - 2025 - Query-Aware Graph Neural Networks for Enhanced Retrieval-Augmented Generation.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\A3PTD9RH\\2508.html}
}

@article{ahmadi2020,
  title = {Mining {{Expressive Rules}} in {{Knowledge Graphs}}},
  author = {Ahmadi, Naser and Huynh, Viet-Phi and Meduri, Vamsi and Ortona, Stefano and Papotti, Paolo},
  year = 2020,
  month = may,
  journal = {J. Data and Information Quality},
  volume = {12},
  number = {2},
  pages = {8:1--8:27},
  issn = {1936-1955},
  doi = {10.1145/3371315},
  urldate = {2026-01-26},
  abstract = {We describe RuDiK, an algorithm and a system for mining declarative rules over RDF knowledge graphs (KGs). RuDiK can discover rules expressing both positive relationships between KG elements, e.g., ``if two persons share at least one parent, they are likely to be siblings,'' and negative patterns identifying data contradictions, e.g., ``if two persons are married, one cannot be the child of the other'' or ``the birth year for a person cannot be bigger than her graduation year.'' While the first kind of rules identify new facts in the KG, the second kind enables the detection of incorrect triples and the generation of (training) negative examples for learning algorithms. High-quality rules are also critical for any reasoning task involving the KGs.Our approach increases the expressive power of the supported rule language w.r.t. the existing systems. RuDiK discovers rules containing (i) comparisons among literal values and (ii) selection conditions with constants. Richer rules increase the accuracy and the coverage over the facts in the KG for the task at hand. This is achieved with aggressive pruning of the search space and with disk-based algorithms, which enable the execution of the system in commodity machines. Also, RuDiK is robust to errors and missing data in the input graph. It discovers approximate rules with a measure of support that is aware of the quality issues. Our experimental evaluation with real-world KGs shows that RuDiK does better than existing solutions in terms of scalability and that it can identify effective rules for different target applications.},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Ahmadi et al. - 2020 - Mining Expressive Rules in Knowledge Graphs.pdf}
}

@book{baader2017,
  title = {An {{Introduction}} to {{Description Logic}}},
  author = {Baader, Franz and Horrocks, Ian and Lutz, Carsten and Sattler, Uli},
  year = 2017,
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/9781139025355},
  urldate = {2026-01-19},
  abstract = {Description logics (DLs) have a long tradition in computer science and knowledge representation, being designed so that domain knowledge can be described and so that computers can reason about this knowledge. DLs have recently gained increased importance since they form the logical basis of widely used ontology languages, in particular the web ontology language OWL. Written by four renowned experts, this is the first textbook on description logics. It is suitable for self-study by graduates and as the basis for a university course. Starting from a basic DL, the book introduces the reader to their syntax, semantics, reasoning problems and model theory and discusses the computational complexity of these reasoning problems and algorithms to solve them. It then explores a variety of reasoning techniques, knowledge-based applications and tools and it describes the relationship between DLs and OWL.},
  isbn = {978-0-521-87361-1},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Baader et al. - 2017 - An Introduction to Description Logic.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\3CX5X2TV\\6D329698AFC2E6C6C5C15801ED9B6D07.html}
}

@article{badreddine2022,
  title = {Logic {{Tensor Networks}}},
  shorttitle = {{{LTN}}},
  author = {Badreddine, Samy and {d'Avila Garcez}, Artur and Serafini, Luciano and Spranger, Michael},
  year = 2022,
  month = feb,
  journal = {Artificial Intelligence},
  volume = {303},
  pages = {103649},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2021.103649},
  urldate = {2026-01-04},
  abstract = {Attempts at combining logic and neural networks into neurosymbolic approaches have been on the increase in recent years. In a neurosymbolic system, symbolic knowledge assists deep learning, which typically uses a sub-symbolic distributed representation, to learn and reason at a higher level of abstraction. We present Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully differentiable logical language, called Real Logic, whereby the elements of a first-order logic signature are grounded onto data using neural computational graphs and first-order fuzzy logic semantics. We show that LTN provides a uniform language to represent and compute efficiently many of the most important AI tasks such as multi-label classification, relational learning, data clustering, semi-supervised learning, regression, embedding learning and query answering. We implement and illustrate each of the above tasks with several simple explanatory examples using TensorFlow 2. The results indicate that LTN can be a general and powerful framework for neurosymbolic AI.},
  keywords = {Deep learning and reasoning,Many-valued logics,Neurosymbolic AI},
  file = {G\:\\My Drive\\Zotero\\Symbolic AI\\Badreddine et al. - 2022 - Logic Tensor Networks.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\ZQRW83ML\\S0004370221002009.html}
}

@inproceedings{barcelo2020,
  title = {The {{Logical Expressiveness}} of {{Graph Neural Networks}}},
  booktitle = {Eighth {{International Conference}} on {{Learning Representations}}},
  author = {Barcel{\'o}, Pablo and Kostylev, Egor V. and Monet, Mikael and P{\'e}rez, Jorge and Reutter, Juan and Silva, Juan Pablo},
  year = 2020,
  month = apr,
  urldate = {2026-02-01},
  abstract = {The ability of graph neural networks (GNNs) for distinguishing nodes in graphs has been recently characterized in terms of the Weisfeiler-Lehman (WL) test for checking graph isomorphism. This characterization, however, does not settle the issue of which Boolean node classifiers (i.e., functions classifying nodes in graphs as true or false) can be expressed by GNNs. We tackle this problem by focusing on Boolean classifiers expressible as formulas in the logic FOC2, a well-studied fragment of first order logic. FOC2 is tightly related to the WL test, and hence to GNNs. We start by studying a popular class of GNNs, which we call AC-GNNs, in which the features of each node in the graph are updated, in successive layers, only in terms of the features of its neighbors. We show that this class of GNNs is too weak to capture all FOC2 classifiers, and provide a syntactic characterization of the largest subclass of FOC2 classifiers that can be captured by AC-GNNs. This subclass coincides with a logic heavily used by the knowledge representation community. We then look at what needs to be added to AC-GNNs for capturing all FOC2 classifiers. We show that it suffices to add readout functions, which allow to update the features of a node not only in terms of its neighbors, but also in terms of a global attribute vector. We call GNNs of this kind ACR-GNNs. We experimentally validate our findings showing that, on synthetic data conforming to FOC2 formulas, AC-GNNs struggle to fit the training data while ACR-GNNs can generalize even to graphs of sizes not seen during training.},
  langid = {english},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Barceló et al. - 2020 - The Logical Expressiveness of Graph Neural Networks.pdf}
}

@misc{barcelo2022,
  title = {Weisfeiler and {{Leman Go Relational}}},
  author = {Barcelo, Pablo and Galkin, Mikhail and Morris, Christopher and Orth, Miguel Romero},
  year = 2022,
  month = nov,
  number = {arXiv:2211.17113},
  eprint = {2211.17113},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.17113},
  urldate = {2026-02-01},
  abstract = {Knowledge graphs, modeling multi-relational data, improve numerous applications such as question answering or graph logical reasoning. Many graph neural networks for such data emerged recently, often outperforming shallow architectures. However, the design of such multi-relational graph neural networks is ad-hoc, driven mainly by intuition and empirical insights. Up to now, their expressivity, their relation to each other, and their (practical) learning performance is poorly understood. Here, we initiate the study of deriving a more principled understanding of multi-relational graph neural networks. Namely, we investigate the limitations in the expressive power of the well-known Relational GCN and Compositional GCN architectures and shed some light on their practical learning performance. By aligning both architectures with a suitable version of the Weisfeiler-Leman test, we establish under which conditions both models have the same expressive power in distinguishing non-isomorphic (multi-relational) graphs or vertices with different structural roles. Further, by leveraging recent progress in designing expressive graph neural networks, we introduce the \$k\$-RN architecture that provably overcomes the expressiveness limitations of the above two architectures. Empirically, we confirm our theoretical findings in a vertex classification setting over small and large multi-relational graphs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Barcelo et al. - 2022 - Weisfeiler and Leman Go Relational.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\4GJJ8PWD\\2211.html}
}

@article{bellomarini2018,
  title = {The {{Vadalog}} System: Datalog-Based Reasoning for Knowledge Graphs},
  shorttitle = {The {{Vadalog}} System},
  author = {Bellomarini, Luigi and Sallinger, Emanuel and Gottlob, Georg},
  year = 2018,
  month = may,
  journal = {Proceedings of the VLDB Endowment},
  volume = {11},
  number = {9},
  pages = {975--987},
  issn = {2150-8097},
  doi = {10.14778/3213880.3213888},
  urldate = {2026-01-15},
  abstract = {Over the past years, there has been a resurgence of Datalog-based systems in the database community as well as in industry. In this context, it has been recognized that to handle the complex knowledge-based scenarios encountered today, such as reasoning over large knowledge graphs, Datalog has to be extended with features such as existential quantification. Yet, Datalog-based reasoning in the presence of existential quantification is in general undecidable. Many efforts have been made to define decidable fragments. Warded Datalog+/- is a very promising one, as it captures PTIME complexity while allowing ontological reasoning. Yet so far, no implementation of Warded Datalog+/- was available. In this paper we present the Vadalog system, a Datalog-based system for performing complex logic reasoning tasks, such as those required in advanced knowledge graphs. The Vadalog system is Oxford's contribution to the VADA research programme, a joint effort of the universities of Oxford, Manchester and Edinburgh and around 20 industrial partners. As the main contribution of this paper, we illustrate the first implementation of Warded Datalog+/-, a high-performance Datalog+/- system utilizing an aggressive termination control strategy. We also provide a comprehensive experimental evaluation.},
  langid = {english},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Bellomarini et al. - 2018 - The Vadalog system datalog-based reasoning for knowledge graphs.pdf}
}

@inproceedings{bellomarini2019,
  title = {Knowledge {{Graphs}} and {{Enterprise AI}}: {{The Promise}} of an {{Enabling Technology}}},
  shorttitle = {Knowledge {{Graphs}} and {{Enterprise AI}}},
  booktitle = {2019 {{IEEE}} 35th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Bellomarini, Luigi and Fakhoury, Daniele and Gottlob, Georg and Sallinger, Emanuel},
  year = 2019,
  month = apr,
  pages = {26--37},
  issn = {2375-026X},
  doi = {10.1109/ICDE.2019.00011},
  urldate = {2026-01-15},
  abstract = {Adopting a mature AI strategy is fundamental for modern knowledge companies to govern the proliferation of smart AI-driven applications and to coordinate them within coherent knowledge workflows. We propose knowledge graphs as the reference technology for the enterprise AI context, i.e., the complex of entities, properties and relationships that shape a business domain and constitute a common backbone for all AI-driven applications. We contribute and discuss principles to design software architectures for AI-driven applications based on knowledge graphs. We focus on the Vadalog system, a successful knowledge graph middleware from the University of Oxford and show knowledge graphs in action in a number of use cases from the financial domain.},
  keywords = {ai-driven applications,Banking,Cognition,Companies,Data models,datalog,enterprise ai,finance,Knowledge engineering,knowledge graph,knowledge graph management system,reasoning}
}

@article{bonatti2019,
  title = {Knowledge {{Graphs}}: {{New Directions}} for {{Knowledge Representation}} on the {{Semantic Web}} ({{Dagstuhl Seminar}} 18371)},
  shorttitle = {Knowledge {{Graphs}}},
  author = {Bonatti, Piero Andrea and Decker, Stefan and Polleres, Axel and Presutti, Valentina},
  editor = {Bonatti, Piero Andrea and Decker, Stefan and Polleres, Axel and Presutti, Valentina},
  year = 2019,
  journal = {Dagstuhl Reports},
  volume = {8},
  number = {9},
  pages = {29--111},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f\"ur Informatik},
  address = {Dagstuhl, Germany},
  issn = {2192-5283},
  doi = {10.4230/DagRep.8.9.29},
  urldate = {2026-01-15},
  keywords = {knowledge graphs,knowledge representation,linked data,ontologies,semantic web},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Bonatti et al. - 2019 - Knowledge Graphs New Directions for Knowledge Representation on the Semantic Web (Dagstuhl Seminar.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\M9ASCK84\\DagRep.8.9.html}
}

@book{brachman-levesque2004,
  title = {Knowledge Representation and Reasoning},
  author = {Brachman, Ronald J.},
  year = 2004,
  publisher = {Amsterdam ; Boston : Morgan Kaufmann},
  urldate = {2026-01-28},
  abstract = {xxix, 381 p. : 24 cm; Includes bibliographical references (p. 349-375) and index},
  collaborator = {{Internet Archive}},
  isbn = {978-1-55860-932-7},
  langid = {english},
  keywords = {Knowledge representation (Information theory)}
}

@article{ceri1989,
  title = {What You Always Wanted to Know about {{Datalog}} (and Never Dared to Ask)},
  author = {Ceri, S. and Gottlob, G. and Tanca, L.},
  year = 1989,
  month = mar,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {1},
  number = {1},
  pages = {146--166},
  issn = {1558-2191},
  doi = {10.1109/69.43410},
  urldate = {2026-01-14},
  abstract = {Datalog, a database query language based on the logic programming paradigm, is described. The syntax and semantics of Datalog and its use for querying a relational database are presented. Optimization methods for achieving efficient evaluations of Datalog queries are classified, and the most relevant methods are presented. Various improvements of Datalog currently under study are discussed, and what is still needed in order to extend Datalog's applicability to the solution of real-life problems is indicated.{$<>$}},
  keywords = {Amplitude shift keying,Artificial intelligence,Books,Database languages,Logic programming,Merging,Optimization methods,Programming environments,Query processing,Relational databases},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Ceri et al. - 1989 - What you always wanted to know about Datalog (and never dared to ask).pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\RHSU6TJQ\\43410.html}
}

@article{chen2020,
  title = {A Review: {{Knowledge}} Reasoning over Knowledge Graph},
  shorttitle = {A Review},
  author = {Chen, Xiaojun and Jia, Shengbin and Xiang, Yang},
  year = 2020,
  month = mar,
  journal = {Expert Syst. Appl.},
  volume = {141},
  number = {C},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2019.112948},
  urldate = {2026-01-15},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Chen et al. - 2020 - A review Knowledge reasoning over knowledge graph.pdf}
}

@misc{cheng2023,
  title = {Neural {{Compositional Rule Learning}} for {{Knowledge Graph Reasoning}}},
  author = {Cheng, Kewei and Ahmed, Nesreen K. and Sun, Yizhou},
  year = 2023,
  month = mar,
  number = {arXiv:2303.03581},
  eprint = {2303.03581},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.03581},
  urldate = {2026-01-19},
  abstract = {Learning logical rules is critical to improving reasoning in KGs. This is due to their ability to provide logical and interpretable explanations when used for predictions, as well as their ability to generalize to other tasks, domains, and data. While recent methods have been proposed to learn logical rules, the majority of these methods are either restricted by their computational complexity and can not handle the large search space of large-scale KGs, or show poor generalization when exposed to data outside the training set. In this paper, we propose an end-to-end neural model for learning compositional logical rules called NCRL. NCRL detects the best compositional structure of a rule body, and breaks it into small compositions in order to infer the rule head. By recurrently merging compositions in the rule body with a recurrent attention unit, NCRL finally predicts a single rule head. Experimental results show that NCRL learns high-quality rules, as well as being generalizable. Specifically, we show that NCRL is scalable, efficient, and yields state-of-the-art results for knowledge graph completion on large-scale KGs. Moreover, we test NCRL for systematic generalization by learning to reason on small-scale observed graphs and evaluating on larger unseen ones.},
  archiveprefix = {arXiv},
  keywords = {citationofcitation,Computer Science - Artificial Intelligence},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Cheng et al. - 2023 - Neural Compositional Rule Learning for Knowledge Graph Reasoning.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\BL3IFI43\\2303.html}
}

@book{cheng2025,
  title = {Knowledge {{Graph Reasoning}}: {{A Neuro-Symbolic Perspective}}},
  shorttitle = {Knowledge {{Graph Reasoning}}},
  author = {Cheng, Kewei and Sun, Yizhou},
  year = 2025,
  series = {Synthesis {{Lectures}} on {{Data}}, {{Semantics}}, and {{Knowledge}}},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-72008-6},
  urldate = {2026-01-04},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-031-72007-9 978-3-031-72008-6},
  langid = {english},
  keywords = {Knowledge Graph Embedding,Knowledge Graph Ontology,Knowledge Graph Reasoning,Logical Query Representation,Neural Symbolic Reasoning},
  file = {G:\My Drive\Zotero\Symbolic AI\Cheng and Sun - 2025 - Knowledge Graph Reasoning A Neuro-Symbolic Perspective.pdf}
}

@inproceedings{cucala2021,
  title = {Explainable {{GNN-Based Models}} over {{Knowledge Graphs}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Cucala, David Jaime Tena and Grau, Bernardo Cuenca and Kostylev, Egor V. and Motik, Boris},
  year = 2021,
  month = oct,
  urldate = {2026-01-15},
  abstract = {Graph Neural Networks (GNNs) are often used to learn transformations of graph data. While effective in practice, such approaches make predictions via numeric manipulations so their output cannot be easily explained symbolically. We propose a new family of GNN-based transformations of graph data that can be trained effectively, but where all predictions can be explained symbolically as logical inferences in Datalog---a well-known rule-based formalism. In particular, we show how to encode an input knowledge graph into a graph with numeric feature vectors, process this graph using a GNN, and decode the result into an output knowledge graph. We use a new class of monotonic GNNs (MGNNs) to ensure that this process is equivalent to a round of application of a set of Datalog rules. We also show that, given an arbitrary MGNN, we can automatically extract rules that completely characterise the transformation. We evaluate our approach by applying it to classification tasks in knowledge graph completion.},
  langid = {english},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Cucala et al. - 2021 - Explainable GNN-Based Models over Knowledge Graphs.pdf}
}

@article{cucala2023,
  title = {On the {{Correspondence Between Monotonic Max-Sum GNNs}} and {{Datalog}}},
  author = {Cucala, David Tena and Grau, Bernardo Cuenca and Motik, Boris and Kostylev, Egor V.},
  year = 2023,
  month = aug,
  journal = {Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning},
  volume = {19},
  number = {1},
  pages = {658--667},
  issn = {2334-1033},
  doi = {10.24963/kr.2023/64},
  urldate = {2026-01-14},
  abstract = {Although there has been significant interest in applying machine learning techniques to structured data, the expressivity (i.e., a description of what can be learned) of such techniques is still po...},
  langid = {english},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Cucala et al. - 2023 - On the Correspondence Between Monotonic Max-Sum GNNs and Datalog.pdf}
}

@book{davilagarcez2009,
  title = {Neural-{{Symbolic Cognitive Reasoning}}},
  author = {{d Avila Garcez}, Artur S. and {Luis C. Lamb} and {Dov M. Gabbay}},
  year = 2009,
  series = {Cognitive {{Technologies}}},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  issn = {1611-2482},
  doi = {10.1007/978-3-540-73246-4},
  urldate = {2026-01-04},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-540-73245-7 978-3-540-73246-4},
  langid = {english},
  keywords = {artificial intelligence,intelligence,knowledge representation,learning,logic,machine learning,modal logic,probabilistic reasoning},
  file = {G:\My Drive\Zotero\Symbolic AI\2009 - Neural-Symbolic Cognitive Reasoning.pdf}
}

@article{davilagarcezValuebasedArgumentationFrameworks2005,
  title = {Value-Based {{Argumentation Frameworks}} as {{Neural-symbolic Learning Systems}}},
  author = {D'Avila Garcez, Artur S. and Gabbay, Dov M. and Lamb, Luis C.},
  year = 2005,
  month = dec,
  journal = {Journal of Logic and Computation},
  volume = {15},
  number = {6},
  pages = {1041--1058},
  issn = {0955-792X},
  doi = {10.1093/logcom/exi057},
  urldate = {2026-01-04},
  abstract = {While neural networks have been successfully used in a number of machine learning applications, logical languages have been the standard for the representation of argumentative reasoning. In this paper, we establish a relationship between neural networks and argumentation networks, combining reasoning and learning in the same argumentation framework. We do so by presenting a new neural argumentation algorithm, responsible for translating argumentation networks into standard neural networks. We then show a correspondence between the two networks. The algorithm works not only for acyclic argumentation networks, but also for circular networks, and it enables the accrual of arguments through learning as well as the parallel computation of arguments.},
  file = {G\:\\My Drive\\Zotero\\Symbolic AI\\D'Avila Garcez et al. - 2005 - Value-based Argumentation Frameworks as Neural-symbolic Learning Systems.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\HDNAHUQX\\exi057.html}
}

@article{delongNeurosymbolicAIReasoning2025,
  title = {Neurosymbolic {{AI}} for {{Reasoning Over Knowledge Graphs}}: {{A Survey}}},
  shorttitle = {Neurosymbolic {{AI}} for {{Reasoning Over Knowledge Graphs}}},
  author = {DeLong, Lauren Nicole and Mir, Ramon Fern{\'a}ndez and Fleuriot, Jacques D.},
  year = 2025,
  month = may,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {36},
  number = {5},
  pages = {7822--7842},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2024.3420218},
  urldate = {2026-01-04},
  abstract = {Neurosymbolic artificial intelligence (AI) is an increasingly active area of research that combines symbolic reasoning methods with deep learning to leverage their complementary benefits. As knowledge graphs (KGs) are becoming a popular way to represent heterogeneous and multirelational data, methods for reasoning on graph structures have attempted to follow this neurosymbolic paradigm. Traditionally, such approaches have utilized either rule-based inference or generated representative numerical embeddings from which patterns could be extracted. However, several recent studies have attempted to bridge this dichotomy to generate models that facilitate interpretability, maintain competitive performance, and integrate expert knowledge. Therefore, we survey methods that perform neurosymbolic reasoning tasks on KGs and propose a novel taxonomy by which we can classify them. Specifically, we propose three major categories: 1) logically informed embedding approaches; 2) embedding approaches with logical constraints; and 3) rule-learning approaches. Alongside the taxonomy, we provide a tabular overview of the approaches and links to their source code, if available, for more direct comparison. Finally, we discuss the unique characteristics and limitations of these methods and then propose several prospective directions toward which this field of research could evolve.},
  keywords = {Artificial intelligence,Artificial neural networks,Cognition,Graph neural networks (GNNs),hybrid artificial intelligence (AI),Knowledge graphs,knowledge graphs (KGs),neurosymbolic AI,representation learning,Semantics,Surveys,Taxonomy},
  file = {G\:\\My Drive\\Zotero\\Symbolic AI\\DeLong et al. - 2025 - Neurosymbolic AI for Reasoning Over Knowledge Graphs A Survey.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\F7TMH2SI\\10603423.html}
}

@inproceedings{dong2014,
  title = {Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion},
  shorttitle = {Knowledge Vault},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Dong, Xin and Gabrilovich, Evgeniy and Heitz, Geremy and Horn, Wilko and Lao, Ni and Murphy, Kevin and Strohmann, Thomas and Sun, Shaohua and Zhang, Wei},
  year = 2014,
  month = aug,
  pages = {601--610},
  publisher = {ACM},
  address = {New York New York USA},
  doi = {10.1145/2623330.2623623},
  urldate = {2026-01-15},
  isbn = {978-1-4503-2956-9},
  langid = {english},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Dong et al. - 2014 - Knowledge vault a web-scale approach to probabilistic knowledge fusion.pdf;G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Knowledge Vault A Web-Scale Approach to Probabilistic Knowledge Fusion.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\UI2GK7ZR\\knowledge-vault-a-web-scale-approach-to-probabilistic-knowledge-fusion.html}
}

@inproceedings{ehrlinger2016,
  title = {Towards a {{Definition}} of {{Knowledge Graphs}}},
  booktitle = {Joint {{Proceedings}} of the {{Posters}} and {{Demos Track}} of the 12th {{International Conference}} on {{Semantic Systems}} - {{SEMANTiCS2016}} and the 1st {{International Workshop}} on {{Semantic Change}} \& {{Evolving Semantics}} ({{SuCCESS}}'16) Co-Located with the 12th {{International Conference}} on {{Semantic Systems}} ({{SEMANTiCS}} 2016), {{Leipzig}}, {{Germany}}, {{September}} 12-15, 2016},
  author = {Ehrlinger, Lisa and W{\"o}{\ss}, Wolfram},
  editor = {Martin, Michael and Cuquet, Mart{\'i} and Folmer, Erwin},
  year = 2016,
  series = {{{CEUR Workshop Proceedings}}},
  volume = {1695},
  publisher = {CEUR-WS.org},
  urldate = {2026-01-15},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Ehrlinger and Wöß - 2016 - Towards a Definition of Knowledge Graphs.pdf}
}

@article{evans2018,
  title = {Learning {{Explanatory Rules}} from {{Noisy Data}}},
  shorttitle = {{{DILP}}},
  author = {Evans, Richard and Grefenstette, Edward},
  year = 2018,
  month = jan,
  journal = {Journal of Artificial Intelligence Research},
  volume = {61},
  pages = {1--64},
  issn = {1076-9757},
  doi = {10.1613/jair.5714},
  urldate = {2026-01-04},
  abstract = {Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data--which is not necessarily easily obtained--that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.},
  copyright = {Copyright (c)},
  langid = {english},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Evans and Grefenstette - 2018 - Learning Explanatory Rules from Noisy Data.pdf}
}

@inproceedings{galarraga2013,
  title = {{{AMIE}}: Association Rule Mining under Incomplete Evidence in Ontological Knowledge Bases},
  shorttitle = {{{AMIE}}},
  booktitle = {Proceedings of the 22nd International Conference on {{World Wide Web}}},
  author = {Gal{\'a}rraga, Luis Antonio and Teflioudi, Christina and Hose, Katja and Suchanek, Fabian},
  year = 2013,
  month = may,
  series = {{{WWW}} '13},
  pages = {413--422},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2488388.2488425},
  urldate = {2026-01-16},
  abstract = {Recent advances in information extraction have led to huge knowledge bases (KBs), which capture knowledge in a machine-readable format. Inductive Logic Programming (ILP) can be used to mine logical rules from the KB. These rules can help deduce and add missing knowledge to the KB. While ILP is a mature field, mining logical rules from KBs is different in two aspects: First, current rule mining systems are easily overwhelmed by the amount of data (state-of-the art systems cannot even run on today's KBs). Second, ILP usually requires counterexamples. KBs, however, implement the open world assumption (OWA), meaning that absent data cannot be used as counterexamples. In this paper, we develop a rule mining model that is explicitly tailored to support the OWA scenario. It is inspired by association rule mining and introduces a novel measure for confidence. Our extensive experiments show that our approach outperforms state-of-the-art approaches in terms of precision and coverage. Furthermore, our system, AMIE, mines rules orders of magnitude faster than state-of-the-art approaches.},
  isbn = {978-1-4503-2035-1},
  keywords = {citationofcitation},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Galárraga et al. - 2013 - AMIE association rule mining under incomplete evidence in ontological knowledge bases 1.pdf;G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Galárraga et al. - 2013 - AMIE association rule mining under incomplete evidence in ontological knowledge bases.pdf}
}

@article{galarraga2015,
  title = {Fast Rule Mining in Ontological Knowledge Bases with {{AMIE}}\$\$+\$\$},
  author = {Gal{\'a}rraga, Luis and Teflioudi, Christina and Hose, Katja and Suchanek, Fabian M.},
  year = 2015,
  month = dec,
  journal = {The VLDB Journal},
  volume = {24},
  number = {6},
  pages = {707--730},
  issn = {0949-877X},
  doi = {10.1007/s00778-015-0394-1},
  urldate = {2026-01-26},
  abstract = {Recent advances in information extraction have led to huge knowledge bases (KBs), which capture knowledge in a machine-readable format. Inductive logic programming (ILP) can be used to mine logical rules from these KBs, such as ``If two persons are married, then they (usually) live in the same city.'' While ILP is a mature field, mining logical rules from KBs is difficult, because KBs make an open-world assumption. This means that absent information cannot be taken as counterexamples. Our approach AMIE~(Gal\'arraga et al. in WWW, 2013) has shown how rules can be mined effectively from KBs even in the absence of counterexamples. In this paper, we show how this approach can be optimized to mine even larger KBs with more than 12M statements. Extensive experiments show how our new approach, AMIE\$\$+\$\$, extends to areas of mining that were previously beyond reach.},
  langid = {english},
  keywords = {ILP,Inductive logic programming,Knowledge bases,Rule mining},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Galárraga et al. - 2015 - Fast rule mining in ontological knowledge bases with AMIE$$+$$.pdf}
}

@misc{galkin2024a,
  title = {Towards {{Foundation Models}} for {{Knowledge Graph Reasoning}}},
  author = {Galkin, Mikhail and Yuan, Xinyu and Mostafa, Hesham and Tang, Jian and Zhu, Zhaocheng},
  year = 2024,
  month = apr,
  number = {arXiv:2310.04562},
  eprint = {2310.04562},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.04562},
  urldate = {2026-02-01},
  abstract = {Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap. The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies. In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. ULTRA builds relational representations as a function conditioned on their interactions. Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph. Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. Fine-tuning further boosts the performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Galkin et al. - 2024 - Towards Foundation Models for Knowledge Graph Reasoning 1.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\3TDKSTF6\\2310.html}
}

@book{genesereth2020,
  title = {Introduction to {{Logic Programming}}},
  author = {Genesereth, Michael and Chaudhri, Vinay K.},
  year = 2020,
  series = {Synthesis {{Lectures}} on {{Artificial Intelligence}} and {{Machine Learning}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-01586-1},
  urldate = {2026-01-07},
  copyright = {https://www.springer.com/tdm},
  isbn = {978-3-031-00458-2 978-3-031-01586-1},
  langid = {english},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Genesereth and Chaudhri - 2020 - Introduction to Logic Programming.pdf}
}

@inproceedings{gilmer2017,
  title = {Neural {{Message Passing}} for {{Quantum Chemistry}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  year = 2017,
  month = jul,
  pages = {1263--1272},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2026-01-15},
  abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  langid = {english},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry 1.pdf;G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf}
}

@misc{grau2025,
  title = {The {{Correspondence Between Bounded Graph Neural Networks}} and {{Fragments}} of {{First-Order Logic}}},
  author = {Grau, Bernardo Cuenca and Feng, Eva and Wa{\l}{\k e}ga, Przemys{\l}aw A.},
  year = 2025,
  month = nov,
  number = {arXiv:2505.08021},
  eprint = {2505.08021},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.08021},
  urldate = {2026-01-15},
  abstract = {Graph Neural Networks (GNNs) address two key challenges in applying deep learning to graph-structured data: they handle varying size input graphs and ensure invariance under graph isomorphism. While GNNs have demonstrated broad applicability, understanding their expressive power remains an important question. In this paper, we propose GNN architectures that correspond precisely to prominent fragments of first-order logic (FO), including various modal logics as well as more expressive two-variable fragments. To establish these results, we apply methods from finite model theory of first-order and modal logics to the domain of graph representation learning. Our results provide a unifying framework for understanding the logical expressiveness of GNNs within FO.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Grau et al. - 2025 - The Correspondence Between Bounded Graph Neural Networks and Fragments of First-Order Logic.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\F5DZHMYB\\2505.html}
}

@inproceedings{guo2016,
  title = {Jointly {{Embedding Knowledge Graphs}} and {{Logical Rules}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Guo, Shu and Wang, Quan and Wang, Lihong and Wang, Bin and Guo, Li},
  editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
  year = 2016,
  month = nov,
  pages = {192--202},
  publisher = {Association for Computational Linguistics},
  address = {Austin, Texas},
  doi = {10.18653/v1/D16-1019},
  urldate = {2026-01-15},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Guo et al. - 2016 - Jointly Embedding Knowledge Graphs and Logical Rules.pdf}
}

@article{hogan2021,
  title = {Knowledge {{Graphs}}},
  author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and D'amato, Claudia and Melo, Gerard De and Gutierrez, Claudio and Kirrane, Sabrina and Gayo, Jos{\'e} Emilio Labra and Navigli, Roberto and Neumaier, Sebastian and Ngomo, Axel-Cyrille Ngonga and Polleres, Axel and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
  year = 2021,
  month = jul,
  journal = {ACM Comput. Surv.},
  volume = {54},
  number = {4},
  pages = {71:1--71:37},
  issn = {0360-0300},
  doi = {10.1145/3447772},
  urldate = {2026-01-15},
  abstract = {In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Hogan et al. - 2021 - Knowledge Graphs.pdf}
}

@book{hogan2022,
  title = {Knowledge {{Graphs}}},
  author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and Damato, Claudia and Melo, Gerard De},
  year = 2022,
  publisher = {Morgan \& Claypool Publishers},
  address = {San Rafael},
  doi = {10.1007/978-3-031-01918-0},
  urldate = {2026-01-15},
  abstract = {This book provides a comprehensive and accessible introduction to knowledge graphs, which have recently garnered notable attention from both industry and academia. Knowledge graphs are founded on the principle of applying a graph-based abstraction to data, and are now broadly deployed in scenarios that require integrating and extracting value from multiple, diverse sources of data at large scale.The book defines knowledge graphs and provides a high-level overview of how they are used. It presents and contrasts popular graph models that are commonly used to represent data as graphs, and the languages by which they can be queried before describing how the resulting data graph can be enhanced with notions of schema, identity, and context. The book discusses how ontologies and rules can be used to encode knowledge as well as how inductive techniques---based on statistics, graph analytics, machine learning, etc.---can be used to encode and extract knowledge. It covers techniques for the creation, enrichment, assessment, and refinement of knowledge graphs and surveys recent open and enterprise knowledge graphs and the industries or applications within which they have been most widely adopted. The book closes by discussing the current limitations and future directions along which knowledge graphs are likely to evolve.This book is aimed at students, researchers, and practitioners who wish to learn more about knowledge graphs and how they facilitate extracting value from diverse data at large scale. To make the book accessible for newcomers, running examples and graphical notation are used throughout. Formal definitions and extensive references are also provided for those who opt to delve more deeply into specific topics.},
  isbn = {978-1-63639-235-6},
  langid = {english},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Hogan et al. - 2022 - Knowledge Graphs.pdf}
}

@misc{huang2023,
  title = {A {{Theory}} of {{Link Prediction}} via {{Relational Weisfeiler-Leman}} on {{Knowledge Graphs}}},
  author = {Huang, Xingyue and Orth, Miguel Romero and Ceylan, {\.I}smail {\.I}lkan and Barcel{\'o}, Pablo},
  year = 2023,
  month = oct,
  number = {arXiv:2302.02209},
  eprint = {2302.02209},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.02209},
  urldate = {2026-01-27},
  abstract = {Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Huang et al. - 2023 - A Theory of Link Prediction via Relational Weisfeiler-Leman on Knowledge Graphs.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\EEJ2AMLA\\2302.html}
}

@book{kejriwal2021,
  title = {Knowledge {{Graphs}}: {{Fundamentals}}, {{Techniques}}, and {{Applications}}},
  shorttitle = {Knowledge {{Graphs}}},
  author = {Kejriwal, Mayank and Knoblock, Craig A. and Szekely, Pedro},
  year = 2021,
  month = mar,
  publisher = {MIT Press},
  abstract = {A rigorous and comprehensive textbook covering the major approaches to knowledge graphs, an active and interdisciplinary area within artificial intelligence.The field of knowledge graphs, which allows us to model, process, and derive insights from complex real-world data, has emerged as an active and interdisciplinary area of artificial intelligence over the last decade, drawing on such fields as natural language processing, data mining, and the semantic web. Current projects involve predicting cyberattacks, recommending products, and even gleaning insights from thousands of papers on COVID-19. This textbook offers rigorous and comprehensive coverage of the field. It focuses systematically on the major approaches, both those that have stood the test of time and the latest deep learning methods.},
  googlebooks = {iqvuDwAAQBAJ},
  isbn = {978-0-262-36188-0},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Data Science / Data Visualization,Computers / Database Administration & Management},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Kejriwal et al. - 2021 - Knowledge Graphs Fundamentals, Techniques, and Applications.pdf}
}

@article{kifer1992,
  title = {Theory of Generalized Annotated Logic Programming and Its Applications*},
  author = {Kifer, Michael and Subrahmanian, V. S.},
  year = 1992,
  month = apr,
  journal = {The Journal of Logic Programming},
  volume = {12},
  number = {4},
  pages = {335--367},
  issn = {0743-1066},
  doi = {10.1016/0743-1066(92)90007-P},
  urldate = {2026-01-10},
  abstract = {Annotated logics were introduced in [43] and later studied in [5, 7, 31, 32]. In [32], annotations were extended to allow variables and functions, and it was argued that such logics can be used to provide a formal semantics for rule-based expert systems with uncertainty. In this paper, we continue to investigate the power of this approach. First, we introduce a new semantics for such programs based on ideals of lattices. Subsequently, some proposals for multivalued logic programming [5, 7, 18, 32, 40, 47] as well as some formalisms for temporal reasoning [1, 3, 41] are shown to fit into this framework. As an interesting byproduct of the investigation, we obtain a new result concerning multivalued logic programming: a model theory for Fitting's bilattice-based logic programming, which until now has not been characterized model-theoretically. This is accompanied by a corresponding proof theory.},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Kifer and Subrahmanian - 1992 - Theory of generalized annotated logic programming and its applications.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\7PD9SK6U\\074310669290007P.html}
}

@inproceedings{kipf2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = 2017,
  month = feb,
  urldate = {2026-01-17},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  langid = {english},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolutional Networks.pdf}
}

@article{kompridis2000,
  title = {So {{We Need Something Else}} for {{Reason}} to {{Mean}}},
  author = {Kompridis, Nikolas},
  year = 2000,
  journal = {International Journal of Philosophical Studies},
  volume = {8},
  number = {3},
  pages = {271--295},
  publisher = {Taylor \& Francis},
  doi = {10.1080/096725500750039282},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Kompridis - 2000 - So We Need Something Else for Reason to Mean.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\E7W2B2RU\\KOMSWN.html}
}

@article{kroger1975,
  title = {Influence of a New Virostatic Compound on the Induction of Enzymes in Rat Liver},
  author = {Kr{\"o}ger, H. and Donner, I. and Skiello, G.},
  year = 1975,
  month = sep,
  journal = {Arzneimittel-Forschung},
  volume = {25},
  number = {9},
  pages = {1426--1429},
  issn = {0004-4172},
  abstract = {The virostatic compound N,N-diethyl-4-[2-(2-oxo-3-tetradecyl-1-imidazolidinyl)-ethyl]-1-piperazinecarboxamide-hydrochloride (5531) was analyzed as to its effect on the induction of tryptophan-pyrrolase and tyrosineaminotransferase in rat liver. 1. The basic activity of the enzymes was not influenced by the substance either in normal or in adrenalectomized animals. 2. The induction of the enzymes by cortisone increased in the presence of the compound whereas the substrate induction remained unchanged. 3. The induction of tyrosine-aminotransferase by dexamethasonephosphate in tissue culture is inhibited if the dose of compound 5531 is higher than 5 mug/ml.},
  langid = {english},
  pmid = {24},
  keywords = {Adrenalectomy,Animals,Antiviral Agents,Cortisone,Culture Techniques,Dose-Response Relationship Drug,Enzyme Induction,Female,Imidazoles,Liver,Piperazines,Rats,Time Factors,Tryptophan Oxygenase,Tyrosine,Tyrosine Transaminase}
}

@article{liangAIReasoningDeep2025,
  title = {{{AI Reasoning}} in {{Deep Learning Era}}: {{From Symbolic AI}} to {{Neural}}--{{Symbolic AI}}},
  shorttitle = {{{AI Reasoning}} in {{Deep Learning Era}}},
  author = {Liang, Baoyu and Wang, Yuchen and Tong, Chao},
  year = 2025,
  month = may,
  journal = {Mathematics},
  volume = {13},
  number = {11},
  pages = {1707},
  issn = {2227-7390},
  doi = {10.3390/math13111707},
  urldate = {2026-01-04},
  abstract = {The pursuit of Artificial General Intelligence (AGI) demands AI systems that not only perceive but also reason in a human-like manner. While symbolic systems pioneered early breakthroughs in logic-based reasoning, such as MYCIN and DENDRAL, they suffered from brittleness and poor scalability. Conversely, modern deep learning architectures have achieved remarkable success in perception tasks, yet continue to fall short in interpretable and structured reasoning. This dichotomy has motivated growing interest in Neural--Symbolic AI, a paradigm that integrates symbolic logic with neural computation to unify reasoning and learning. This survey provides a comprehensive and technically grounded overview of AI reasoning in the deep learning era, with a particular focus on Neural--Symbolic AI. Beyond a historical narrative, we introduce a formal definition of AI reasoning and propose a novel three-dimensional taxonomy that organizes reasoning paradigms by representation form, task structure, and application context. We then systematically review recent advances---including Differentiable Logic Programming, abductive learning, program induction, logic-aware Transformers, and LLM-based symbolic planning---highlighting their technical mechanisms, capabilities, and limitations. In contrast to prior surveys, this work bridges symbolic logic, neural computation, and emergent generative reasoning, offering a unified framework to understand and compare diverse approaches. We conclude by identifying key open challenges such as symbolic--continuous alignment, dynamic rule learning, and unified architectures, and we aim to provide a conceptual foundation for future developments in general-purpose reasoning systems.},
  langid = {english},
  file = {G:\My Drive\Zotero\Symbolic AI\Liang et al. - 2025 - AI Reasoning in Deep Learning Era From Symbolic AI to Neural–Symbolic AI.pdf}
}

@inproceedings{liu2021,
  title = {{{INDIGO}}: {{GNN-Based Inductive Knowledge Graph Completion Using Pair-Wise Encoding}}},
  shorttitle = {{{INDIGO}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Shuwen and Grau, Bernardo and Horrocks, Ian and Kostylev, Egor},
  year = 2021,
  volume = {34},
  pages = {2034--2045},
  publisher = {Curran Associates, Inc.},
  urldate = {2026-01-16},
  keywords = {citationofcitation},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Liu et al. - 2021 - INDIGO GNN-Based Inductive Knowledge Graph Completion Using Pair-Wise Encoding.pdf}
}

@article{liu2023,
  title = {Learning {{Rule-Induced Subgraph Representations}} for {{Inductive Relation Prediction}}},
  author = {Liu, Tianyu and Lv, Qitan and Wang, Jie and Yang, Shuling and Chen, Hanzhu},
  year = 2023,
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {3517--3535},
  urldate = {2026-01-16},
  langid = {english},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Liu et al. - 2023 - Learning Rule-Induced Subgraph Representations for Inductive Relation Prediction.pdf}
}

@article{liu2025,
  title = {Knowledge Reasoning Based on Graph Neural Networks with Multi-Layer Top-p Message Passing and Sparse Negative Sampling},
  author = {Liu, Wenjie and Ren, Zhijie and Chen, Liang},
  year = 2025,
  month = feb,
  journal = {Knowledge-Based Systems},
  volume = {311},
  pages = {113063},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2025.113063},
  urldate = {2026-01-25},
  abstract = {Graph Neural Networks (GNNs) have received increasing interest in knowledge reasoning since they can learn the structure and semantic information of graphs. However, as the number of hops increases, the number of entities grows exponentially, resulting in a sharp rise in resource costs. Furthermore, the presence of irrelevant entities during message passing can accumulate noise, thereby diminishing the model's accuracy. To solve these problems, a knowledge reasoning model based on graph neural networks with multi-layer top-p message passing and sparse negative sampling is proposed. To be specific, we designed a dynamic top-p message-passing strategy that dynamically samples key entities related to the query based on their probability distribution, thereby reducing high resource costs. Then the similarity-based negative sampling is applied to dense entities in the knowledge graph, while random sampling is used for sparse entities, which enhances the model's ability to identify irrelevant entities. Extensive experiments conducted on three datasets (WN18RR, FB15k-237, and NELL-995) demonstrate our model outperforms other SOTA methods in knowledge reasoning, achieving an average improvement of 5.16\%, 6.16\% and 6.71\% in MRR, Hits@1 and Hits@10 respectively, and it also has a great advantage in training efficiency than SOTA GNN-based methods, with the average improvement of 11.99\% in training time. Our model not only offers a novel method for knowledge reasoning, but also contributes valuable insights for the advancement of knowledge graph technologies.},
  keywords = {Graph neural networks,Knowledge reasoning,Multi-layer top-p message passing,Sparse negative sampling},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Liu et al. - 2025 - Knowledge reasoning based on graph neural networks with multi-layer top-p message passing and sparse.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\MFQZ6YIX\\S0950705125001108.html}
}

@book{lloyd1987,
  title = {Foundations of Logic Programming; (2nd Extended Ed.): \textbar{} {{Guide}} Books \textbar{} {{ACM Digital Library}}},
  shorttitle = {Foundations of Logic Programming; (2nd Extended Ed.)},
  author = {Lloyd, J. W.},
  year = 1987,
  month = jan,
  publisher = {Springer-Verlag},
  address = {Berlin},
  urldate = {2026-01-14},
  isbn = {978-3-540-18199-6},
  langid = {english},
  file = {C:\Users\Surface\Documents\zotero\storage\4S4NPLMY\39279.html}
}

@article{manhaeve,
  title = {{{DeepProbLog}}:  {{Neural Probabilistic Logic Programming}}},
  shorttitle = {{{DeepProbLog}}},
  author = {Manhaeve, Robin and Dumancic, Sebastijan and Kimmig, Angelika and Demeester, Thomas and Raedt, Luc De},
  abstract = {We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.},
  langid = {english},
  file = {G:\My Drive\Zotero\Symbolic AI\Manhaeve et al. - DeepProbLog  Neural Probabilistic Logic Programming.pdf}
}

@article{manhaeve2021,
  title = {Neural Probabilistic Logic Programming in {{DeepProbLog}}},
  author = {Manhaeve, Robin and Duman{\v c}i{\'c}, Sebastijan and Kimmig, Angelika and Demeester, Thomas and De Raedt, Luc},
  year = 2021,
  journal = {Artificial intelligence},
  volume = {298},
  pages = {103504-},
  publisher = {Elsevier B.V},
  address = {Amsterdam},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2021.103504},
  abstract = {We introduce DeepProbLog, a neural probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques of the underlying probabilistic logic programming language ProbLog can be adapted for the new language. We theoretically and experimentally demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.},
  langid = {english},
  keywords = {Computer science,Inference,Logic,Logic programming},
  file = {G:\My Drive\Zotero\Symbolic AI\Manhaeve et al. - 2021 - Neural probabilistic logic programming in DeepProbLog.pdf}
}

@misc{mao2019,
  title = {The {{Neuro-Symbolic Concept Learner}}: {{Interpreting Scenes}}, {{Words}}, and {{Sentences From Natural Supervision}}},
  shorttitle = {The {{Neuro-Symbolic Concept Learner}}},
  author = {Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum, Joshua B. and Wu, Jiajun},
  year = 2019,
  month = apr,
  number = {arXiv:1904.12584},
  eprint = {1904.12584},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.12584},
  urldate = {2026-01-04},
  abstract = {We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Mao et al. - 2019 - The Neuro-Symbolic Concept Learner Interpreting Scenes, Words, and Sentences From Natural Supervisi.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\YPYNJRUM\\1904.html}
}

@article{marraStatisticalRelationalNeurosymbolic2024,
  title = {From Statistical Relational to Neurosymbolic Artificial Intelligence: {{A}} Survey},
  shorttitle = {From Statistical Relational to Neurosymbolic Artificial Intelligence},
  author = {Marra, Giuseppe and Duman{\v c}i{\'c}, Sebastijan and Manhaeve, Robin and De Raedt, Luc},
  year = 2024,
  month = mar,
  journal = {Artificial Intelligence},
  volume = {328},
  pages = {104062},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2023.104062},
  urldate = {2026-01-04},
  abstract = {This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neurosymbolic and statistical relational artificial intelligence. Neurosymbolic artificial intelligence (NeSy) studies the integration of symbolic reasoning and neural networks, while statistical relational artificial intelligence (StarAI) focuses on integrating logic with probabilistic graphical models. This survey identifies seven shared dimensions between these two subfields of AI. These dimensions can be used to characterize different NeSy and StarAI systems. They are concerned with (1) the approach to logical inference, whether model or proof-based; (2) the syntax of the used logical theories; (3) the logical semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either parameter or structure learning; (5) the presence of symbolic and subsymbolic representations; (6) the degree to which systems capture the original logic, probabilistic, and neural paradigms; and (7) the classes of learning tasks the systems are applied to. By positioning various NeSy and StarAI systems along these dimensions and pointing out similarities and differences between them, this survey contributes fundamental concepts for understanding the integration of learning and reasoning.},
  keywords = {Learning and reasoning,Neurosymbolic AI,Probabilistic logics,Statistical relational AI},
  file = {G\:\\My Drive\\Zotero\\Symbolic AI\\Marra et al. - 2024 - From statistical relational to neurosymbolic artificial intelligence A survey.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\FUMYHL2D\\S0004370223002084.html}
}

@article{mccarthy2006,
  title = {A {{Proposal}} for the {{Dartmouth Summer Research Project}} on {{Artificial Intelligence}}, {{August}} 31, 1955},
  author = {McCarthy, John and Minsky, Marvin L. and Rochester, Nathaniel and Shannon, Claude E.},
  year = 2006,
  month = dec,
  journal = {AI Magazine},
  volume = {27},
  number = {4},
  pages = {12--12},
  issn = {2371-9621},
  doi = {10.1609/aimag.v27i4.1904},
  urldate = {2026-01-27},
  abstract = {The 1956 Dartmouth summer research project on artificial intelligence was initiated by this August 31, 1955 proposal, authored by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. The original typescript consisted of 17 pages plus a title page. Copies of the typescript are housed in the archives at Dartmouth College and Stanford University. The first 5 papers state the proposal, and the remaining pages give qualifications and interests of the four who proposed the study. In the interest of brevity, this article reproduces only the proposal itself, along with the short autobiographical statements of the proposers.},
  copyright = {Copyright (c)},
  langid = {english},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\McCarthy et al. - 2006 - A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955.pdf}
}

@inproceedings{morris2025,
  title = {Sound {{Logical Explanations}} for {{Mean Aggregation Graph Neural Networks}}},
  booktitle = {The {{Thirty-ninth Annual Conference}} on {{Neural Information Processing Systems}}},
  author = {Morris, Matthew and Horrocks, Ian},
  year = 2025,
  month = oct,
  urldate = {2026-01-25},
  abstract = {Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.},
  langid = {english},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Morris and Horrocks - 2025 - Sound Logical Explanations for Mean Aggregation Graph Neural Networks.pdf}
}

@misc{morris2025a,
  title = {Levels of {{AGI}} for {{Operationalizing Progress}} on the {{Path}} to {{AGI}}},
  author = {Morris, Meredith Ringel and {Sohl-Dickstein}, Jascha and Fiedel, Noah and Warkentin, Tris and Dafoe, Allan and Faust, Aleksandra and Farabet, Clement and Legg, Shane},
  year = 2025,
  month = sep,
  number = {arXiv:2311.02462},
  eprint = {2311.02462},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.02462},
  urldate = {2026-01-26},
  abstract = {We propose a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy, providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop our framework, we analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. With these principles in mind, we propose "Levels of AGI" based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. We discuss the challenging requirements for future benchmarks that quantify the behavior and capabilities of AGI models against these levels. Finally, we discuss how these levels of AGI interact with deployment considerations such as autonomy and risk, and emphasize the importance of carefully selecting Human-AI Interaction paradigms for responsible and safe deployment of highly capable AI systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Morris et al. - 2025 - Levels of AGI for Operationalizing Progress on the Path to AGI.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\ZYDU32HZ\\2311.html}
}

@book{naqvi,
  title = {A Logical Language for Data and Knowledge Bases: \textbar{} {{Guide}} Books \textbar{} {{ACM Digital Library}}},
  shorttitle = {A Logical Language for Data and Knowledge Bases},
  author = {Naqvi, Shamim and Tsur, Shalom},
  urldate = {2026-01-14},
  langid = {english},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\A logical language for data and knowledge bases  Guide books  ACM Digital Library.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\N8DNHYVA\\63511.html}
}

@book{neisser1967,
  title = {Cognitive Psychology},
  author = {Neisser, Ulric},
  year = 1967,
  publisher = {New York : Appleton-Century-Crofts},
  urldate = {2026-01-27},
  abstract = {xi, 351 pages : 24 cm; Includes bibliographical references (pages 309-333); The cognitive approach -- Iconic storage and verbal coding -- Pattern recognition -- Focal attention and figural synthesis -- Words as visual patterns -- Visual memory -- Speech perception -- Echoic memory and auditory attention -- Active verbal memory -- Sentences -- A cognitive approach to memory and thought},
  collaborator = {{Internet Archive}},
  isbn = {978-0-390-66509-6 978-0-13-139667-8},
  langid = {english},
  keywords = {Cognition}
}

@article{noy2019,
  title = {Industry-Scale {{Knowledge Graphs}}: {{Lessons}} and {{Challenges}}},
  shorttitle = {Industry-Scale {{Knowledge Graphs}}},
  author = {Noy, Natasha and {Search about this author} and Gao, Yuqing and {Search about this author} and Jain, Anshu and {Search about this author} and Narayanan, Anant and {Search about this author} and Patterson, Alan and {Search about this author} and Taylor, Jamie and {Search about this author}},
  year = 2019,
  month = apr,
  journal = {Queue},
  volume = {17},
  number = {2},
  pages = {48--75},
  publisher = {Association for Computing Machinery},
  doi = {10.1145/3329781.3332266},
  urldate = {2026-01-15},
  abstract = {This article looks at the knowledge graphs of five diverse tech companies, comparing the similarities and differences in their respective experiences of building and using the graphs, and discussing the challenges that all knowledge-driven enterprises face today. The collection of knowledge graphs discussed here covers the breadth of applications, from search, to product descriptions, to social networks.},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Noy et al. - 2019 - Industry-scale Knowledge Graphs Lessons and Challenges.pdf}
}

@inproceedings{pan2024,
  title = {A {{Symbolic Rule Integration Framework}} with {{Logic Transformer}} for {{Inductive Relation Prediction}}},
  booktitle = {Proceedings of the {{ACM Web Conference}} 2024},
  author = {Pan, Yudai and Liu, Jun and Zhao, Tianzhe and Zhang, Lingling and Lin, Yun and Dong, Jin Song},
  year = 2024,
  month = may,
  series = {{{WWW}} '24},
  pages = {2181--2192},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3589334.3645594},
  urldate = {2026-01-16},
  abstract = {Relation prediction in knowledge graphs (KGs) aims at predicting missing relations in incomplete triples, whereas the dominant paradigm by KG embeddings has a limitation to predict the relation between unseen entities. This situation is called an inductive setting, which is more common in the real-world scenario. To handle this issue, implicit symbolic rules have shown great potential in capturing the inductive capability. However, it is still challenging to obtain precise representations of logic rules from KGs. The argument variability and predicate non-commutativity in symbolic rule integration make the modeling of component symbols difficult. To this end, we propose a novel inductive relation prediction model named SymRITa with a logic transformer integrating rules. SymRITa firstly extracts the subgraph, whose embeddings are captured by a graph network. Meanwhile, symbolic rule graphs in the subgraph can be generated. Then, the symbolic rules are modeled by a proposed logic transformer. Specifically, the input format based on the subgraph-based embeddings is to focus on the argument variability in symbolic rules. In addition, a conjunction attention mechanism in the logic transformer can resolve predicate non-commutativity in the symbolic rule integration process. Finally, the subgraph-based and symbol-based embeddings obtained from the previous steps are combined for the training regime, and prediction results as well as rules explaining the reasoning process are explicitly output. Extensive experiments on twelve inductive datasets show that SymRITa achieves outstanding effectiveness compared to state-of-the-art inductive baselines. Moreover, the logic rules with corresponding confidences provide an interpretable paradigm.},
  isbn = {979-8-4007-0171-9},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Pan et al. - 2024 - A Symbolic Rule Integration Framework with Logic Transformer for Inductive Relation Prediction.pdf}
}

@article{paulheim2016,
  title = {Knowledge Graph Refinement: {{A}} Survey of Approaches and Evaluation Methods},
  shorttitle = {Knowledge Graph Refinement},
  author = {Paulheim, Heiko},
  editor = {Cimiano, Philipp},
  year = 2016,
  month = dec,
  journal = {Semantic Web},
  volume = {8},
  number = {3},
  pages = {489--508},
  issn = {22104968, 15700844},
  doi = {10.3233/SW-160218},
  urldate = {2026-01-15},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Paulheim - 2016 - Knowledge graph refinement A survey of approaches and evaluation methods.pdf}
}

@misc{renQuery2boxReasoningKnowledge2020,
  title = {Query2box: {{Reasoning}} over {{Knowledge Graphs}} in {{Vector Space}} Using {{Box Embeddings}}},
  shorttitle = {Query2box},
  author = {Ren, Hongyu and Hu, Weihua and Leskovec, Jure},
  year = 2020,
  month = feb,
  number = {arXiv:2002.05969},
  eprint = {2002.05969},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.05969},
  urldate = {2026-01-04},
  abstract = {Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions (\$\textbackslash wedge\$) and existential quantifiers (\$\textbackslash exists\$). Handling queries with logical disjunctions (\$\textbackslash vee\$) remains an open problem. Here we propose query2box, an embedding-based framework for reasoning over arbitrary queries with \$\textbackslash wedge\$, \$\textbackslash vee\$, and \$\textbackslash exists\$ operators in massive and incomplete KGs. Our main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. We show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KG entities. However, we show that by transforming queries into a Disjunctive Normal Form, query2box is capable of handling arbitrary logical queries with \$\textbackslash wedge\$, \$\textbackslash vee\$, \$\textbackslash exists\$ in a scalable manner. We demonstrate the effectiveness of query2box on three large KGs and show that query2box achieves up to 25\% relative improvement over the state of the art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G\:\\My Drive\\Zotero\\Symbolic AI\\Ren et al. - 2020 - Query2box Reasoning over Knowledge Graphs in Vector Space using Box Embeddings.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\RHG27XLM\\2002.html}
}

@misc{riegel2020,
  title = {Logical {{Neural Networks}}},
  author = {Riegel, Ryan and Gray, Alexander and Luus, Francois and Khan, Naweed and Makondo, Ndivhuwo and Akhalwaya, Ismail Yunus and Qian, Haifeng and Fagin, Ronald and Barahona, Francisco and Sharma, Udit and Ikbal, Shajith and Karanam, Hima and Neelam, Sumit and Likhyani, Ankita and Srivastava, Santosh},
  year = 2020,
  month = jun,
  number = {arXiv:2006.13155},
  eprint = {2006.13155},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.13155},
  urldate = {2026-01-10},
  abstract = {We propose a novel framework seamlessly providing key properties of both neural nets (learning) and symbolic logic (knowledge and reasoning). Every neuron has a meaning as a component of a formula in a weighted real-valued logic, yielding a highly intepretable disentangled representation. Inference is omnidirectional rather than focused on predefined target variables, and corresponds to logical reasoning, including classical first-order logic theorem proving as a special case. The model is end-to-end differentiable, and learning minimizes a novel loss function capturing logical contradiction, yielding resilience to inconsistent knowledge. It also enables the open-world assumption by maintaining bounds on truth values which can have probabilistic semantics, yielding resilience to incomplete knowledge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Riegel et al. - 2020 - Logical Neural Networks.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\PEUFZDYK\\2006.html}
}

@inproceedings{rudolph2008,
  title = {Description {{Logic Reasoning}} with {{Decision Diagrams}}},
  booktitle = {The {{Semantic Web}} - {{ISWC}} 2008},
  author = {Rudolph, Sebastian and Kr{\"o}tzsch, Markus and Hitzler, Pascal},
  editor = {Sheth, Amit and Staab, Steffen and Dean, Mike and Paolucci, Massimo and Maynard, Diana and Finin, Timothy and Thirunarayan, Krishnaprasad},
  year = 2008,
  pages = {435--450},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-88564-1_28},
  abstract = {We propose a novel method for reasoning in the description logic \$\textbackslash mathcal\textbraceleft SHIQ\textbraceright\$. After a satisfiability preserving transformation from \$\textbackslash mathcal\textbraceleft SHIQ\textbraceright\$to the description logic \$\textbackslash mathcal\textbraceleft ALCI\textbraceright b\$, the obtained \$\textbackslash mathcal\textbraceleft ALCI\textbraceright b\$Tbox \$\textbackslash mathcal\textbraceleft T\textbraceright\$is converted into an ordered binary decision diagram (OBDD) which represents a canonical model for \$\textbackslash mathcal\textbraceleft T\textbraceright\$. This OBDD is turned into a disjunctive datalog program that can be used for Abox reasoning. The algorithm is worst-case optimal w.r.t. data complexity, and admits easy extensions with DL-safe rules and ground conjunctive queries.},
  isbn = {978-3-540-88564-1},
  langid = {english},
  keywords = {Binary Decision Diagram,Boolean Function,Conjunctive Query,Description Logic,Negation Normal Form},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Rudolph et al. - 2008 - Description Logic Reasoning with Decision Diagrams.pdf}
}

@misc{RusselNorvig2022,
  title = {Artificial {{Intelligence}}: {{A Modern Approach}}, 4th {{US}} Ed.},
  author = {{Stuart Russel} and {Peter Norvig}},
  year = 2022,
  month = aug,
  urldate = {2026-01-27},
  howpublished = {https://aima.cs.berkeley.edu/},
  file = {C:\Users\Surface\Documents\zotero\storage\WXKTS889\aima.cs.berkeley.edu.html}
}

@incollection{sadeghian2019,
  title = {{{DRUM}}: End-to-End Differentiable Rule Mining on Knowledge Graphs},
  shorttitle = {{{DRUM}}},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Sadeghian, Ali and Armandpour, Mohammadreza and Ding, Patrick and Wang, Daisy Zhe},
  year = 2019,
  month = dec,
  number = {1375},
  pages = {15347--15357},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2026-01-19},
  abstract = {In this paper, we study the problem of learning probabilistic logical rules for inductive and interpretable link prediction. Despite the importance of inductive link prediction, most previous works focused on transductive link prediction and cannot manage previously unseen entities. Moreover, they are black-box models that are not easily explainable for humans. We propose DRUM, a scalable and differentiable approach for mining first-order logical rules from knowledge graphs which resolves these problems. We motivate our method by making a connection between learning confidence scores for each rule and low-rank tensor approximation. DRUM uses bidirectional RNNs to share useful information across the tasks of learning rules for different relations. We also empirically demonstrate the efficiency of DRUM over existing rule mining methods for inductive link prediction on a variety of benchmark datasets.},
  keywords = {citationofcitation},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Sadeghian et al. - 2019 - DRUM end-to-end differentiable rule mining on knowledge graphs.pdf}
}

@inproceedings{sakharov2024,
  title = {Fuzzy {{Logic Based}} on {{Zadeh Operators}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Progress}} in {{Informatics}} and {{Computing}} ({{PIC}})},
  author = {Sakharov, Alexander},
  year = 2024,
  month = dec,
  pages = {80--84},
  issn = {2329-6259},
  doi = {10.1109/PIC62406.2024.10892715},
  urldate = {2026-01-07},
  abstract = {Zadeh operators for conjunction, disjunction, and negation are not considered a good semantic basis for any mathematical logic. In this paper, fuzzy models are specified by Zadeh operators along with a recursive definition of the truth values of implication formulas (as opposed to standard implication truth functions). These models are characterized by a subset of classical first-order logic. This subset is specified as a sequent calculus whose negation rule is different from classical negation inference rules. The recursive implication truth function and the logical characterization justify the use of Zadeh operators in fuzzy systems.},
  keywords = {Calculus,classical first-order logic,Computational modeling,Fuzzy logic,fuzzy model,Fuzzy systems,Informatics,Logic,Mathematical models,Semantics,sequent calculus,Software,Standards,Zadeh operator},
  file = {C:\Users\Surface\Documents\zotero\storage\5WZ6J89J\10892715.html}
}

@book{sarkerNeuroSymbolicArtificialIntelligence2022,
  title = {Neuro-{{Symbolic Artificial Intelligence}}: {{The State}} of the {{Art}}},
  shorttitle = {Neuro-{{Symbolic Artificial Intelligence}}},
  author = {Sarker, Md Kamruzzaman},
  year = 2022,
  month = jan,
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  volume = {00342},
  publisher = {SAGE Publications Ltd},
  address = {Amsterdam, Netherlands},
  urldate = {2026-01-04},
  abstract = {Neuro-symbolic AI is an emerging subfield of Artificial Intelligence that brings together two hitherto distinct approaches. ''Neuro'' refers to the artificial neural networks prominent in machine learning, ''symbolic'' refers to algorithmic processing on the level of meaningful symbols, prominent in knowledge representation. In the past, these two fields of AI have been largely separate, with very little crossover, but the so-called ``third wave'' of AI is now bringing them together. This book, Neuro-Symbolic Artificial Intelligence: The State of the Art, provides an overview of this development in AI. The two approaches differ significantly in terms of their strengths and weaknesses and, from a cognitive-science perspective, there is a question as to how a neural system can perform symbol manipulation, and how the representational differences between these two approaches can be bridged. The book presents 17 overview papers, all by authors who have made significant contributions in the past few years and starting with a historic overview first seen in 2016. With just seven months elapsed from invitation to authors to final copy, the book is as up-to-date as a published overview of this subject can be. Based on the editors'own desire to understand the current state of the art, this book reflects the breadth and depth of the latest developments in neuro-symbolic AI, and will be of interest to students, researchers, and all those working in the field of Artificial Intelligence.},
  isbn = {978-1-64368-244-0},
  langid = {english},
  keywords = {Artificial intelligence,Computational intelligence,COMPUTERS / Artificial Intelligence / General},
  file = {G:\My Drive\Zotero\Symbolic AI\Sarker - 2022 - Neuro-Symbolic Artificial Intelligence The State of the Art.pdf}
}

@article{scarselli2009,
  title = {The {{Graph Neural Network Model}}},
  author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  year = 2009,
  month = jan,
  journal = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {1},
  pages = {61--80},
  issn = {1941-0093},
  doi = {10.1109/TNN.2008.2005605},
  urldate = {2026-01-16},
  abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
  keywords = {Biological system modeling,Biology,Chemistry,Computer vision,Data engineering,Data mining,graph neural networks (GNNs),graph processing,Graphical domains,Neural networks,Parameter estimation,Pattern recognition,recursive neural networks,Supervised learning},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Scarselli et al. - 2009 - The Graph Neural Network Model.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\Y7CENFD4\\4700287.html}
}

@inproceedings{schlichtkrull2018,
  title = {Modeling {{Relational Data}} with {{Graph Convolutional Networks}}},
  booktitle = {The {{Semantic Web}}},
  author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and {van~den Berg}, Rianne and Titov, Ivan and Welling, Max},
  editor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, Rapha{\"e}l and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
  year = 2018,
  pages = {593--607},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-93417-4_38},
  abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e.~subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8\% on FB15k-237 over a decoder-only baseline.},
  isbn = {978-3-319-93417-4},
  langid = {english},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Schlichtkrull et al. - 2018 - Modeling Relational Data with Graph Convolutional Networks.pdf}
}

@article{serafiniLogicTensorNetworks2016,
  title = {Logic {{Tensor Networks}}: {{Deep Learning}} and {{Logical Reasoning}} from {{Data}} and {{Knowledge}}},
  shorttitle = {Logic {{Tensor Networks}}},
  author = {Serafini, Luciano and d'Avila Garcez, Artur},
  year = 2016,
  month = jun,
  journal = {arXiv.org},
  urldate = {2026-01-04},
  abstract = {We propose Logic Tensor Networks: a uniform framework for integrating automatic learning and reasoning. A logic formalism called Real Logic is defined on a first-order language whereby formulas have truth-value in the interval [0,1] and semantics defined concretely on the domain of real numbers. Logical constants are interpreted as feature vectors of real numbers. Real Logic promotes a well-founded integration of deductive reasoning on a knowledge-base and efficient data-driven relational machine learning. We show how Real Logic can be implemented in deep Tensor Neural Networks with the use of Google's tensorflow primitives. The paper concludes with experiments applying Logic Tensor Networks on a simple but representative example of knowledge completion.},
  langid = {english},
  file = {G:\My Drive\Zotero\Symbolic AI\Serafini and Garcez - 2016 - Logic Tensor Networks Deep Learning and Logical Reasoning from Data and Knowledge.pdf}
}

@misc{shakarian2023,
  title = {Extensions to {{Generalized Annotated Logic}} and an {{Equivalent Neural Architecture}}},
  author = {Shakarian, Paulo and Simari, Gerardo I.},
  year = 2023,
  month = feb,
  number = {arXiv:2302.12195},
  eprint = {2302.12195},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.12195},
  urldate = {2026-01-10},
  abstract = {While deep neural networks have led to major advances in image recognition, language translation, data mining, and game playing, there are well-known limits to the paradigm such as lack of explainability, difficulty of incorporating prior knowledge, and modularity. Neuro symbolic hybrid systems have recently emerged as a straightforward way to extend deep neural networks by incorporating ideas from symbolic reasoning such as computational logic. In this paper, we propose a list desirable criteria for neuro symbolic systems and examine how some of the existing approaches address these criteria. We then propose an extension to generalized annotated logic that allows for the creation of an equivalent neural architecture comprising an alternate neuro symbolic hybrid. However, unlike previous approaches that rely on continuous optimization for the training process, our framework is designed as a binarized neural network that uses discrete optimization. We provide proofs of correctness and discuss several of the challenges that must be overcome to realize this framework in an implemented system.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Shakarian and Simari - 2023 - Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\A7WSYFDR\\2302.html}
}

@book{shakarian2023a,
  title = {Neuro {{Symbolic Reasoning}} and {{Learning}}},
  author = {Shakarian, Paulo and Baral, Chitta and Simari, Gerardo I. and Xi, Bowen and Pokala, Lahari},
  year = 2023,
  series = {{{SpringerBriefs}} in {{Computer Science}}},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-39179-8},
  urldate = {2026-01-04},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-031-39178-1 978-3-031-39179-8},
  langid = {english},
  keywords = {Answer Set Programming,Artificial intelligence,Deep Learning,Fuzzy Logic,Inductive logic programming,Knowledge Representation and Reasoning,Logic programming,Machine Learning,Natural language processing,Neural networks,Neuro symbolic artificial intelligence,Reinforcement learning,Symbol grounding,Symbolic artificial intelligence,Visual question answering},
  file = {G:\My Drive\Zotero\Symbolic AI\Shakarian et al. - 2023 - Neuro Symbolic Reasoning and Learning.pdf}
}

@article{shengyuanDifferentiableNeuroSymbolicReasoning2023,
  title = {Differentiable {{Neuro-Symbolic Reasoning}} on {{Large-Scale Knowledge Graphs}}},
  author = {Shengyuan, Chen and Cai, Yunfeng and Fang, Huang and Huang, Xiao and Sun, Mingming},
  year = 2023,
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {28139--28154},
  urldate = {2026-01-04},
  langid = {english},
  file = {G:\My Drive\Zotero\Symbolic AI\Shengyuan et al. - 2023 - Differentiable Neuro-Symbolic Reasoning on Large-Scale Knowledge Graphs.pdf}
}

@article{shindo2021,
  title = {Differentiable {{Inductive Logic Programming}} for {{Structured Examples}}},
  author = {Shindo, Hikaru and Nishino, Masaaki and Yamamoto, Akihiro},
  year = 2021,
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {6},
  pages = {5034--5041},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v35i6.16637},
  urldate = {2026-01-13},
  abstract = {The differentiable implementation of logic yields a seamless combination of symbolic reasoning and deep neural networks. Recent research, which has developed a differentiable framework to learn logic programs from examples, can even acquire reasonable solutions from noisy datasets. However, this framework severely limits expressions for solutions, e.g., no function symbols are allowed, and the shapes of clauses are fixed. As a result, the framework cannot deal with structured examples. Therefore we propose a new framework to learn logic programs from noisy and structured examples, including the following contributions. First, we propose an adaptive clause search method by looking through structured space, which is defined by the generality of the clauses, to yield an efficient search space for differentiable solvers. Second, we propose for ground atoms an enumeration algorithm, which determines a necessary and sufficient set of ground atoms to perform differentiable inference functions. Finally, we propose a new method to compose logic programs softly, enabling the system to deal with complex programs consisting of several clauses. Our experiments show that our new framework can learn logic programs from noisy and structured examples, such as sequences or trees. Our framework can be scaled to deal with complex programs that consist of several clauses with function symbols.},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Shindo et al. - 2021 - Differentiable Inductive Logic Programming for Structured Examples 1.pdf}
}

@misc{singhal2012,
  type = {Blogpost},
  title = {Introducing the {{Knowledge Graph}}: Things, Not Strings},
  shorttitle = {Introducing the {{Knowledge Graph}}},
  author = {Singhal, Amit},
  year = 2012,
  month = may,
  journal = {Google},
  urldate = {2026-01-15},
  abstract = {We hope this will give you a more complete picture of your interest, provide smarter search results, and pique your curiosity.},
  howpublished = {https://blog.google/products-and-platforms/products/search/introducing-knowledge-graph-things-not/},
  langid = {american},
  file = {C:\Users\Surface\Documents\zotero\storage\RSCAFETZ\introducing-knowledge-graph-things-not.html}
}

@misc{soeteman2026,
  title = {Logical {{Expressiveness}} of {{Graph Neural Networks}} with {{Hierarchical Node Individualization}}},
  author = {Soeteman, Arie and ten Cate, Balder},
  year = 2026,
  month = jan,
  number = {arXiv:2506.13911},
  eprint = {2506.13911},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.13911},
  urldate = {2026-02-01},
  abstract = {We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an expressive extension of graph neural networks (GNNs) with hierarchical node individualization, inspired by the Individualization-Refinement paradigm for isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy of increasingly expressive models that, in the limit, distinguish graphs up to isomorphism. We show that, over graphs of bounded degree, the separating power of HEGNN node classifiers equals that of graded hybrid logic. This characterization enables us to relate the separating power of HEGNNs to that of higher-order GNNs, GNNs enriched with local homomorphism count features, and color refinement algorithms based on Individualization-Refinement. Our experimental results confirm the practical feasibility of HEGNNs and show benefits in comparison with traditional GNN architectures, both with and without local homomorphism count features.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Soeteman and Cate - 2026 - Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\IZX9VH8I\\2506.html}
}

@article{tarski1955,
  title = {A Lattice-Theoretical Fixpoint Theorem and Its Applications},
  author = {Tarski, Alfred},
  year = 1955,
  month = jun,
  journal = {Pacific Journal of Mathematics},
  volume = {5},
  number = {2},
  pages = {285--309},
  publisher = {Mathematical Sciences Publishers},
  issn = {0030-8730},
  urldate = {2026-01-12},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Tarski - 1955 - A lattice-theoretical fixpoint theorem and its applications.pdf}
}

@article{turing1950,
  title = {I.---{{COMPUTING MACHINERY AND INTELLIGENCE}}},
  author = {TURING, A. M.},
  year = 1950,
  month = oct,
  journal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  urldate = {2026-01-27},
  file = {C:\Users\Surface\Documents\zotero\storage\9WBKY7IJ\LIX.236.html}
}

@article{vanemden1976,
  title = {The {{Semantics}} of {{Predicate Logic}} as a {{Programming Language}}},
  author = {Van Emden, M. H. and Kowalski, R. A.},
  year = 1976,
  month = oct,
  journal = {J. ACM},
  volume = {23},
  number = {4},
  pages = {733--742},
  issn = {0004-5411},
  doi = {10.1145/321978.321991},
  urldate = {2026-01-12},
  abstract = {Sentences in first-order predicate logic can be usefully interpreted as programs. In this paper the operational and fixpoint semantics of predicate logic programs are defined, and the connections with the proof theory and model theory of logic are investigated. It is concluded that operational semantics is a part of proof theory and that fixpoint semantics is a special case of model-theoretic semantics.},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Van Emden and Kowalski - 1976 - The Semantics of Predicate Logic as a Programming Language.pdf}
}

@article{vankrieken2022,
  title = {Analyzing {{Differentiable Fuzzy Logic Operators}}},
  author = {{van Krieken}, Emile and Acar, Erman and {van Harmelen}, Frank},
  year = 2022,
  month = jan,
  journal = {Artificial Intelligence},
  volume = {302},
  number = {103602},
  pages = {1--46},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2021.103602},
  urldate = {2026-01-10},
  abstract = {The AI community is increasingly putting its attention towards combining symbolic and neural approaches, as it is often argued that the strengths and weaknesses of these approaches are complementary. One recent trend in the literature is weakly supervised learning techniques that employ operators from fuzzy logics. In particular, these use prior background knowledge described in such logics to help the training of a neural network from unlabeled and noisy data. By interpreting logical symbols using neural networks, this background knowledge can be added to regular loss functions, hence making reasoning a part of learning. We study, both formally and empirically, how a large collection of logical operators from the fuzzy logic literature behave in a differentiable learning setting. We find that many of these operators, including some of the most well-known, are highly unsuitable in this setting. A further finding concerns the treatment of implication in these fuzzy logics, and shows a strong imbalance between gradients driven by the antecedent and the consequent of the implication. Furthermore, we introduce a new family of fuzzy implications (called sigmoidal implications) to tackle this phenomenon. Finally, we empirically show that it is possible to use Differentiable Fuzzy Logics for semi-supervised learning, and compare how different operators behave in practice. We find that, to achieve the largest performance improvement over a supervised baseline, we have to resort to non-standard combinations of logical operators which perform well in learning, but no longer satisfy the usual logical laws.},
  keywords = {Fuzzy logic,Learning with constraints,Neural-symbolic AI},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\van Krieken et al. - 2022 - Analyzing Differentiable Fuzzy Logic Operators.pdf}
}

@misc{velickovic2018,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = 2018,
  month = feb,
  number = {arXiv:1710.10903},
  eprint = {1710.10903},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.10903},
  urldate = {2026-02-01},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Veličković et al. - 2018 - Graph Attention Networks.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\UCF567BK\\1710.html}
}

@article{wang2014,
  title = {Knowledge {{Graph Embedding}} by {{Translating}} on {{Hyperplanes}}},
  author = {Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
  year = 2014,
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {28},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v28i1.8870},
  urldate = {2026-01-15},
  abstract = {We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {TransH},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Wang et al. - 2014 - Knowledge Graph Embedding by Translating on Hyperplanes.pdf}
}

@article{wang2024,
  title = {Faithful {{Rule Extraction}} for {{Differentiable Rule Learning Models}}},
  author = {Wang, Xiaxia and Tena Cucala, David Jaime and Grau, Bernardo and Horrocks, Ian},
  year = 2024,
  month = may,
  journal = {International Conference on Representation Learning},
  volume = {2024},
  pages = {14769--14791},
  urldate = {2026-01-19},
  langid = {english},
  keywords = {citationofcitation},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Wang et al. - 2024 - Faithful Rule Extraction for Differentiable Rule Learning Models.pdf}
}

@article{wang2025,
  title = {Rule-{{Guided Graph Neural Networks}} for {{Explainable Knowledge Graph Reasoning}}},
  author = {Wang, Zhe and Ma, Suxue and Wang, Kewen and Zhuang, Zhiqiang},
  year = 2025,
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {39},
  number = {12},
  pages = {12784--12791},
  issn = {2374-3468},
  doi = {10.1609/aaai.v39i12.33394},
  urldate = {2026-01-04},
  abstract = {The connections between symbolic rules and neural networks have been explored in various directions, including rule mining through neural networks and rule-based explanation for neural networks. These approaches allow symbolic rules to be extracted from neural network models, which offers explainability to the models. However, the plausibility of the extracted rules is rarely analysed. In this paper, we show that the confidence degrees of extracted rules are generally not high, and we propose a new family of Graph Neural Networks that can be trained with the guidance of rules. Hence, the inference of our model simulates the rule reasoning. Moreover, rules with high confidence degrees can be extracted from the trained model that aligns with the inference of the model, which verifies the effectiveness of the rule guidance. Experimental evaluation of knowledge graph reasoning tasks further demonstrates the effectiveness of our model.},
  copyright = {Copyright (c) 2025 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {G:\My Drive\Zotero\Symbolic AI\Wang et al. - 2025 - Rule-Guided Graph Neural Networks for Explainable Knowledge Graph Reasoning.pdf}
}

@article{wu2019,
  title = {A {{Comprehensive Survey}} on {{Graph Neural Networks}}},
  author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  year = 2019,
  month = jan,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {1},
  eprint = {1901.00596},
  primaryclass = {cs},
  pages = {4--24},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2020.2978386},
  urldate = {2026-01-15},
  abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Wu et al. - 2021 - A Comprehensive Survey on Graph Neural Networks.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\GAE4I9TF\\1901.html}
}

@misc{xuSemanticLossFunction2018,
  title = {A {{Semantic Loss Function}} for {{Deep Learning}} with {{Symbolic Knowledge}}},
  author = {Xu, Jingyi and Zhang, Zilu and Friedman, Tal and Liang, Yitao and den Broeck, Guy Van},
  year = 2018,
  month = jun,
  number = {arXiv:1711.11157},
  eprint = {1711.11157},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.11157},
  urldate = {2026-01-04},
  abstract = {This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that it effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G\:\\My Drive\\Zotero\\Symbolic AI\\Xu et al. - 2018 - A Semantic Loss Function for Deep Learning with Symbolic Knowledge.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\X9M79YP8\\1711.html}
}

@inproceedings{yang2017,
  title = {Differentiable Learning of Logical Rules for Knowledge Base Reasoning},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Yang, Fan and Yang, Zhilin and Cohen, William W.},
  year = 2017,
  month = dec,
  series = {{{NIPS}}'17},
  pages = {2316--2325},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2026-02-01},
  abstract = {We study the problem of learning probabilistic first-order logical rules for knowledge base reasoning. This learning problem is difficult because it requires learning the parameters in a continuous space as well as the structure in a discrete space. We propose a framework, Neural Logic Programming, that combines the parameter and structure learning of first-order logical rules in an end-to-end differentiable model. This approach is inspired by a recently-developed differentiable logic called TensorLog [5], where inference tasks can be compiled into sequences of differentiable operations. We design a neural controller system that learns to compose these operations. Empirically, our method outperforms prior work on multiple knowledge base benchmark datasets, including Freebase and WikiMovies.},
  isbn = {978-1-5108-6096-4}
}

@article{zhang2021,
  title = {Neural, Symbolic and Neural-Symbolic Reasoning on Knowledge Graphs},
  author = {Zhang, Jing and Chen, Bo and Zhang, Lingxi and Ke, Xirui and Ding, Haipeng},
  year = 2021,
  month = jan,
  journal = {AI Open},
  volume = {2},
  pages = {14--35},
  issn = {2666-6510},
  doi = {10.1016/j.aiopen.2021.03.001},
  urldate = {2026-02-02},
  abstract = {Knowledge graph reasoning is the fundamental component to support machine learning applications such as information extraction, information retrieval, and recommendation. Since knowledge graphs can be viewed as the discrete symbolic representations of knowledge, reasoning on knowledge graphs can naturally leverage the symbolic techniques. However, symbolic reasoning is intolerant of the ambiguous and noisy data. On the contrary, the recent advances of deep learning have promoted neural reasoning on knowledge graphs, which is robust to the ambiguous and noisy data, but lacks interpretability compared to symbolic reasoning. Considering the advantages and disadvantages of both methodologies, recent efforts have been made on combining the two reasoning methods. In this survey, we take a thorough look at the development of the symbolic, neural and hybrid reasoning on knowledge graphs. We survey two specific reasoning tasks --- knowledge graph completion and question answering on knowledge graphs, and explain them in a unified reasoning framework. We also briefly discuss the future directions for knowledge graph reasoning.},
  keywords = {Knowledge graph embedding,Knowledge graph reasoning,Neural-symbolic reasoning,Symbolic reasoning},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Zhang et al. - 2021 - Neural, symbolic and neural-symbolic reasoning on knowledge graphs 1.pdf;G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Zhang et al. - 2021 - Neural, symbolic and neural-symbolic reasoning on knowledge graphs.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\Z48DWIEL\\S2666651021000061.html}
}

@misc{zhou2025,
  title = {Double {{Equivariance}} for {{Inductive Link Prediction}} for {{Both New Nodes}} and {{New Relation Types}}},
  author = {Zhou, Jincheng and Zhang, Yucheng and Gao, Jianfei and Zhou, Yangze and Ribeiro, Bruno},
  year = 2025,
  month = jan,
  number = {arXiv:2302.01313},
  eprint = {2302.01313},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.01313},
  urldate = {2026-01-27},
  abstract = {The task of fully inductive link prediction in knowledge graphs has gained significant attention, with various graph neural networks being proposed to address it. This task presents greater challenges than traditional inductive link prediction tasks with only new nodes, as models must be capable of zero-shot generalization to both unseen nodes and unseen relation types in the inference graph. Despite the development of novel models, a unifying theoretical understanding of their success remains elusive, and the limitations of these methods are not well-studied. In this work, we introduce the concept of double permutation-equivariant representations and demonstrate its necessity for effective performance in this task. We show that many existing models, despite their diverse architectural designs, conform to this framework. However, we also identify inherent limitations in double permutation-equivariant representations, which restrict these models's ability to learn effectively on datasets with varying characteristics. Our findings suggest that while double equivariance is necessary for meta-learning across knowledge graphs from different domains, it is not sufficient. There remains a fundamental gap between double permutation-equivariant models and the concept of foundation models designed to learn patterns across all domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Zhou et al. - 2025 - Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\9CFH2LGX\\2302.html}
}

@misc{zhu2022,
  title = {Neural-{{Symbolic Models}} for {{Logical Queries}} on {{Knowledge Graphs}}},
  author = {Zhu, Zhaocheng and Galkin, Mikhail and Zhang, Zuobai and Tang, Jian},
  year = 2022,
  month = may,
  journal = {arXiv.org},
  urldate = {2026-01-25},
  abstract = {Answering complex first-order logic (FOL) queries on knowledge graphs is a fundamental task for multi-hop reasoning. Traditional symbolic methods traverse a complete knowledge graph to extract the answers, which provides good interpretation for each step. Recent neural methods learn geometric embeddings for complex queries. These methods can generalize to incomplete knowledge graphs, but their reasoning process is hard to interpret. In this paper, we propose Graph Neural Network Query Executor (GNN-QE), a neural-symbolic model that enjoys the advantages of both worlds. GNN-QE decomposes a complex FOL query into relation projections and logical operations over fuzzy sets, which provides interpretability for intermediate variables. To reason about the missing links, GNN-QE adapts a graph neural network from knowledge graph completion to execute the relation projections, and models the logical operations with product fuzzy logic. Experiments on 3 datasets show that GNN-QE significantly improves over previous state-of-the-art models in answering FOL queries. Meanwhile, GNN-QE can predict the number of answers without explicit supervision, and provide visualizations for intermediate variables.},
  howpublished = {https://arxiv.org/abs/2205.10128v2},
  langid = {english},
  file = {G:\My Drive\Zotero\AI\Symbolic AI\Zhu et al. - 2022 - Neural-Symbolic Models for Logical Queries on Knowledge Graphs.pdf}
}

@misc{zhu2022a,
  title = {Neural {{Bellman-Ford Networks}}: {{A General Graph Neural Network Framework}} for {{Link Prediction}}},
  shorttitle = {Neural {{Bellman-Ford Networks}}},
  author = {Zhu, Zhaocheng and Zhang, Zuobai and Xhonneux, Louis-Pascal and Tang, Jian},
  year = 2022,
  month = jan,
  number = {arXiv:2106.06935},
  eprint = {2106.06935},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.06935},
  urldate = {2026-02-01},
  abstract = {Link prediction is a very fundamental task on graphs. Inspired by traditional path-based methods, in this paper we propose a general and flexible representation learning framework based on paths for link prediction. Specifically, we define the representation of a pair of nodes as the generalized sum of all path representations, with each path representation as the generalized product of the edge representations in the path. Motivated by the Bellman-Ford algorithm for solving the shortest path problem, we show that the proposed path formulation can be efficiently solved by the generalized Bellman-Ford algorithm. To further improve the capacity of the path formulation, we propose the Neural Bellman-Ford Network (NBFNet), a general graph neural network framework that solves the path formulation with learned operators in the generalized Bellman-Ford algorithm. The NBFNet parameterizes the generalized Bellman-Ford algorithm with 3 neural components, namely INDICATOR, MESSAGE and AGGREGATE functions, which corresponds to the boundary condition, multiplication operator, and summation operator respectively. The NBFNet is very general, covers many traditional path-based methods, and can be applied to both homogeneous graphs and multi-relational graphs (e.g., knowledge graphs) in both transductive and inductive settings. Experiments on both homogeneous graphs and knowledge graphs show that the proposed NBFNet outperforms existing methods by a large margin in both transductive and inductive settings, achieving new state-of-the-art results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Zhu et al. - 2022 - Neural Bellman-Ford Networks A General Graph Neural Network Framework for Link Prediction.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\VV8S567Z\\2106.html}
}

@misc{zhu2023,
  title = {A*{{Net}}: {{A Scalable Path-based Reasoning Approach}} for {{Knowledge Graphs}}},
  shorttitle = {A*{{Net}}},
  author = {Zhu, Zhaocheng and Yuan, Xinyu and Galkin, Mikhail and Xhonneux, Sophie and Zhang, Ming and Gazeau, Maxime and Tang, Jian},
  year = 2023,
  month = nov,
  number = {arXiv:2206.04798},
  eprint = {2206.04798},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.04798},
  urldate = {2026-02-02},
  abstract = {Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A* algorithm for shortest path problems, our A*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10\% nodes and 10\% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A*Net is the first path-based method for knowledge graph reasoning at such scale.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {G\:\\My Drive\\Zotero\\AI\\Symbolic AI\\Zhu et al. - 2023 - ANet A Scalable Path-based Reasoning Approach for Knowledge Graphs.pdf;C\:\\Users\\Surface\\Documents\\zotero\\storage\\FGFABHBC\\2206.html}
}
